{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression as SKLogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import expit\n",
    "from scipy.optimize import minimize_scalar\n",
    "import copy\n",
    "from numpy import ma\n",
    "import numpy.linalg\n",
    "import cupy.linalg\n",
    "import time\n",
    "\n",
    "LOAD_FROM_PICKLE = True\n",
    "USE_SHRUNK_DATASET = True\n",
    "USE_GPU = True\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_GPU:\n",
    "    xp = cp\n",
    "else:\n",
    "    xp = np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "if LOAD_FROM_PICKLE:\n",
    "    with open('../Data/Pickle/cover_data.pickle', 'rb') as handle:\n",
    "        data = xp.load(handle, allow_pickle=True)\n",
    "\n",
    "    print('Loaded data from pickle')\n",
    "else:\n",
    "    data = xp.loadtxt('../Data/Cov_Type/covtype.data', delimiter=',')\n",
    "    with open('../Data/Pickle/cover_data.pickle', 'wb') as handle:\n",
    "        xp.save(handle, data, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Data shape: {}'.format(data.shape))\n",
    "\n",
    "# used for faster testing\n",
    "if USE_SHRUNK_DATASET:\n",
    "    # get the number of samples for the class witht the least samples\n",
    "    min_samples = xp.bincount(data[:, -1].astype(int), minlength=7)[1:].min()\n",
    "\n",
    "    # get min number of samples for each class\n",
    "    data = xp.concatenate([data[data[:, -1] == i][:min_samples] for i in range(1, 8)])\n",
    "\n",
    "    print('New data shape:', data.shape)\n",
    "\n",
    "# split the data into features and labels\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "# normalize the features\n",
    "X = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0) + .0000001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X.shape =', X.shape)\n",
    "print('y.shape =', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_GPU:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, stratify=y.get())\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lr_sk = SKLogisticRegression(solver='liblinear') # all params default\n",
    "\n",
    "if USE_GPU:\n",
    "    lr_sk.fit(X_train.get(), y_train.get())\n",
    "    yhat = lr_sk.predict(X_test.get())\n",
    "    print('Accuracy of: ',accuracy_score(y_test.get(), yhat))\n",
    "else:\n",
    "    lr_sk.fit(X_train, y_train)\n",
    "    yhat = lr_sk.predict(X_test)\n",
    "    print('Accuracy of: ',accuracy_score(y_test, yhat))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper accuracy function\n",
    "def print_accuracy(y, yhat):\n",
    "    if USE_GPU:\n",
    "        print('Accuracy of: ', round(accuracy_score(y_test.get(), yhat.get()) * 100, 3) , '%')\n",
    "    else:\n",
    "        print('Accuracy of: ', round(accuracy_score(y_test, yhat) * 100 , 3), '%')\n",
    "\n",
    "def get_accuracy(y, yhat):\n",
    "    if USE_GPU:\n",
    "        return round(accuracy_score(y_test.get(), yhat.get()) * 100, 3)\n",
    "    else:\n",
    "        return round(accuracy_score(y_test, yhat) * 100 , 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegression:\n",
    "    # private:\n",
    "    def __init__(self, eta, solver='base', iterations=20, C1=0.0, C2=0.0, line_iters=0, batch_size=0):\n",
    "        self.eta = eta\n",
    "        self.solver = solver\n",
    "        self.iters = iterations\n",
    "        self.C1 = C1\n",
    "        self.C2 = C2\n",
    "        self.line_iters = line_iters\n",
    "        self.batch_size = batch_size\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "\n",
    "        solvers = ['base', 'line_search', 'stochastic', 'mini_batch', 'newton']\n",
    "        if solver not in solvers:\n",
    "            raise ValueError('solver %s is not one of %s' % (solver, solvers))\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "    \n",
    "    # convenience, private and static:\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return xp.hstack((xp.ones((X.shape[0], 1)), X)) # add bias term\n",
    "\n",
    "    # this defines the function with the first input to be optimized\n",
    "    # therefore eta will be optimized, with all inputs constant\n",
    "    @staticmethod\n",
    "    def _line_search_objective_function(eta, X, y, w, grad):\n",
    "        wnew = w - grad * eta\n",
    "        g = expit(X @ wnew)\n",
    "\n",
    "        if USE_GPU:\n",
    "            g = g.get()\n",
    "            y = y.get()\n",
    "\n",
    "        # has to be run on the CPU because of the use of the ma module\n",
    "        return -np.sum(ma.log(g[y == 1])) - ma.sum(np.log(1 - g[y == 0]))\n",
    "\n",
    "    def _add_regularization(self, grad):\n",
    "        L1 = self.C1 * xp.sign(self.w_[1:])\n",
    "        L2 = self.C2 -2 * self.w_[1:]\n",
    "        grad[1:] += L1 + L2\n",
    "\n",
    "        return grad\n",
    "\n",
    "    def _get_gradient(self, X, y):\n",
    "        match self.solver:\n",
    "            case 'base' | 'line_search':\n",
    "                ydiff = y - self.predict_proba(X, add_bias=False).ravel() # get y difference\n",
    "                # scale ydiff by ratio of positive to negative samples\n",
    "                # ydiff *= (y == 1).sum() / (y == 0).sum() * 0.5\n",
    "                gradient = xp.mean(X * ydiff[:, xp.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "            case 'stochastic':\n",
    "                idx = int(np.random.rand() * len(y)) # grab random instance\n",
    "                ydiff = y[idx] - self.predict_proba(X[idx], add_bias=False) # get y difference\n",
    "                gradient = X[idx] * ydiff[:, xp.newaxis] # make ydiff a column vector and multiply through\n",
    "            case 'mini_batch':\n",
    "                idx = np.random.choice(len(y), size=self.batch_size, replace=False) # grab random instance\n",
    "                ydiff = y[idx] - self.predict_proba(X[idx], add_bias=False).ravel() # get y difference\n",
    "                gradient = xp.mean(X[idx] * ydiff[:, xp.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "            case 'newton':\n",
    "                g = self.predict_proba(X, add_bias=False).ravel() # get sigmoid value for all classes\n",
    "                \n",
    "                # the hessian has no L1 regularization and L2 will only be included if C2 is not 0\n",
    "                hessian = (X * (g * (1 - g))[:, xp.newaxis]).T @ X - (2 * self.C2) # calculate the hessian\n",
    "\n",
    "                # I swapped X.T @ np.diag(g*(1-g)) with (X * (g*(1-g))[:, xp.newaxis]).T\n",
    "                # They work the same but the latter does the diagonal multiplication using way less memory\n",
    "                # The reduction in memory allows the use of the GPU for the hessian calculation\n",
    "                # otherwise my GPU runs out of memory\n",
    "\n",
    "                ydiff = y - g # get y difference\n",
    "                gradient = xp.sum(X * ydiff[:, np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape) # make gradient a column vector\n",
    "        gradient = self._add_regularization(gradient)\n",
    "\n",
    "        # the hessian is special\n",
    "        if self.solver == 'newton':\n",
    "            return xp.linalg.pinv(hessian) @ gradient\n",
    "        else:\n",
    "            return gradient\n",
    "    \n",
    "    # public:\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = xp.zeros((num_features, 1)) # init weight vector to zeros\n",
    "        \n",
    "        match self.solver:\n",
    "            case 'base' | 'stochastic' | 'mini_batch' | 'newton':\n",
    "                # for as many as the max iterations\n",
    "                for _ in range(self.iters):\n",
    "                    gradient = self._get_gradient(Xb, y)\n",
    "                    self.w_ += gradient * self.eta # multiply by learning rate\n",
    "\n",
    "            case 'line_search':\n",
    "                for _ in range(self.iters):\n",
    "                    gradient = -self._get_gradient(Xb, y)\n",
    "                    # minimization inopposite direction\n",
    "                    \n",
    "                    # do line search in gradient direction, using scipy function\n",
    "                    opts = {'maxiter':self.line_iters} # unclear exactly what this should be\n",
    "                    res = minimize_scalar(self._line_search_objective_function, # objective function to optimize\n",
    "                                        bounds=(0,self.eta * 10), #bounds to optimize\n",
    "                                        args=(Xb, y, self.w_, gradient), # additional argument for objective function\n",
    "                                        method='bounded', # bounded optimization for speed\n",
    "                                        options=opts) # set max iterations\n",
    "                    \n",
    "                    eta = res.x # get optimal learning rate\n",
    "                    self.w_ -= gradient * eta # set new function values\n",
    "                    # subtract to minimize\n",
    "\n",
    "        \n",
    "\n",
    "    def predict_proba(self, X, add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X) > 0.5) #return the actual prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, eta, solver='base', iterations=20, C1=0.0, C2=0.0, line_iters=5, batch_size=5):\n",
    "        self.eta = eta\n",
    "        self.solver = solver\n",
    "        self.iters = iterations\n",
    "        self.C1 = C1\n",
    "        self.C2 = C2\n",
    "        self.line_iters = line_iters\n",
    "        self.batch_size = batch_size\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "\n",
    "        solvers = ['base', 'line_search', 'stochastic', 'mini_batch', 'newton']\n",
    "        if solver not in solvers:\n",
    "            raise ValueError('solver %s is not one of %s' % (solver, solvers))\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = xp.unique(y) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        # create a classifier for each class\n",
    "        for _ in range(num_unique_classes):\n",
    "            blr = BinaryLogisticRegression(self.eta, self.solver, self.iters, self.C1, self.C2, self.line_iters, self.batch_size)\n",
    "            self.classifiers_.append(blr)\n",
    "\n",
    "        # print('Classifiers created')\n",
    "\n",
    "        # train each classifier\n",
    "        for blr, yval, i in zip(self.classifiers_, self.unique_, range(num_unique_classes)):\n",
    "            \n",
    "            y_binary = (y == yval)\n",
    "            blr.fit(X, y_binary)\n",
    "            # print('Classifier Fitted %d of %d' % (i + 1, num_unique_classes), end='\\r')\n",
    "        # print('All Classifiers Fitted        ')\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = xp.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        probs = []\n",
    "        for blr in self.classifiers_:\n",
    "            probs.append(blr.predict_proba(X)) # get probability for each classifier\n",
    "        \n",
    "        return xp.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return self.unique_[xp.argmax(self.predict_proba(X), axis=1)] # take argmax along row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(.1, solver='base', iterations=500, C1=0.01, C2=0.01)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "yhat = lr.predict(X_test)\n",
    "\n",
    "print_accuracy(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(.1, solver='line_search', iterations=100, C1=0.01, C2=0.01, line_iters=2)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "yhat = lr.predict(X_test)\n",
    "\n",
    "print_accuracy(y_test, yhat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(.1, solver='line_search', iterations=100, C1=0.01, C2=0.01, line_iters=5)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "yhat = lr.predict(X_test)\n",
    "\n",
    "print_accuracy(y_test, yhat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(.001, solver='stochastic', iterations=1000, C1=0.01, C2=0.01)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "yhat = lr.predict(X_test)\n",
    "\n",
    "print_accuracy(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(.1, solver='mini_batch', iterations=100, C1=0.01, C2=0.01, batch_size=2000)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "yhat = lr.predict(X_test)\n",
    "\n",
    "print_accuracy(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(.1, solver='newton', iterations=5, C1=0.01, C2=0.01)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "yhat = lr.predict(X_test)\n",
    "\n",
    "print_accuracy(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solvers = ['base', 'line_search', 'stochastic', 'mini_batch', 'newton']\n",
    "solvers = ['newton']\n",
    "base_num_iters = [100, 20, 200, 20, 1] # each solver has a different number of base iterations\n",
    "iter_multipliers = [1, 5, 10]\n",
    "etas = [.001, .01, .1]\n",
    "Cs = [(0.0, 0.0), (0.01, 0.0), (0.0, 0.01), (0.01, 0.01)]\n",
    "\n",
    "results = []\n",
    "\n",
    "for solver in solvers:\n",
    "    for iter_multiplier in iter_multipliers:\n",
    "        for eta in etas:\n",
    "            for C1, C2 in Cs:\n",
    "                # time the training in milliseconds\n",
    "                start = round(time.time() * 1000)\n",
    "\n",
    "                lr = LogisticRegression(eta, solver=solver, iterations=(base_num_iters[solvers.index(solver)] * iter_multiplier), C1=C1, C2=C2)\n",
    "                lr.fit(X_train, y_train)\n",
    "                yhat = lr.predict(X_test)\n",
    "\n",
    "                end = round(time.time() * 1000)\n",
    "\n",
    "                settings = solver + ' eta: ' + str(eta) + ' C1: ' + str(C1) + ' C2: ' + str(C2) + ' iters: ' + str(base_num_iters[solvers.index(solver)] * iter_multiplier)\n",
    "                accuracy = get_accuracy(y_test, yhat)\n",
    "                time_taken = end - start\n",
    "\n",
    "                # store the settings, accuracy, and time\n",
    "                print(settings + ' accuracy: ' + str(accuracy) + ' time: ' + str(time_taken) + 'ms')\n",
    "                results.append((settings, accuracy, time_taken))\n",
    "\n",
    "\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97cc609b13305c559618ec78a438abc56230b9381f827f22d070313b9a1f3777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
