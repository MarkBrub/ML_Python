{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression as SKLogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import expit\n",
    "from scipy.optimize import minimize_scalar\n",
    "import copy\n",
    "from numpy import ma\n",
    "import numpy.linalg\n",
    "import cupy.linalg\n",
    "import time\n",
    "\n",
    "LOAD_FROM_PICKLE = True\n",
    "USE_SHRUNK_DATASET = True\n",
    "USE_GPU = True\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_GPU:\n",
    "    xp = cp\n",
    "else:\n",
    "    xp = np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from pickle\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "if LOAD_FROM_PICKLE:\n",
    "    with open('../Data/Pickle/cover_data.pickle', 'rb') as handle:\n",
    "        data = xp.load(handle, allow_pickle=True)\n",
    "\n",
    "    print('Loaded data from pickle')\n",
    "else:\n",
    "    data = xp.loadtxt('../Data/Cov_Type/covtype.data', delimiter=',')\n",
    "    with open('../Data/Pickle/cover_data.pickle', 'wb') as handle:\n",
    "        xp.save(handle, data, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (581012, 55)\n",
      "New data shape: (19229, 55)\n"
     ]
    }
   ],
   "source": [
    "print('Data shape: {}'.format(data.shape))\n",
    "\n",
    "# used for faster testing\n",
    "if USE_SHRUNK_DATASET:\n",
    "    # get the number of samples for the class witht the least samples\n",
    "    min_samples = xp.bincount(data[:, -1].astype(int), minlength=7)[1:].min()\n",
    "\n",
    "    # get min number of samples for each class\n",
    "    data = xp.concatenate([data[data[:, -1] == i][:min_samples] for i in range(1, 8)])\n",
    "\n",
    "    print('New data shape:', data.shape)\n",
    "\n",
    "# split the data into features and labels\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "# normalize the features\n",
    "X = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0) + .0000001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape = (19229, 54)\n",
      "y.shape = (19229,)\n"
     ]
    }
   ],
   "source": [
    "print('X.shape =', X.shape)\n",
    "print('y.shape =', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_GPU:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, stratify=y.get())\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of:  0.6781071242849714\n",
      "CPU times: user 372 ms, sys: 149 ms, total: 522 ms\n",
      "Wall time: 291 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lr_sk = SKLogisticRegression(solver='liblinear') # all params default\n",
    "\n",
    "if USE_GPU:\n",
    "    lr_sk.fit(X_train.get(), y_train.get())\n",
    "    yhat = lr_sk.predict(X_test.get())\n",
    "    print('Accuracy of: ',accuracy_score(y_test.get(), yhat))\n",
    "else:\n",
    "    lr_sk.fit(X_train, y_train)\n",
    "    yhat = lr_sk.predict(X_test)\n",
    "    print('Accuracy of: ',accuracy_score(y_test, yhat))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper accuracy function\n",
    "def print_accuracy(y, yhat):\n",
    "    if USE_GPU:\n",
    "        print('Accuracy of: ', round(accuracy_score(y_test.get(), yhat.get()) * 100, 3) , '%')\n",
    "    else:\n",
    "        print('Accuracy of: ', round(accuracy_score(y_test, yhat) * 100 , 3), '%')\n",
    "\n",
    "def get_accuracy(y, yhat):\n",
    "    if USE_GPU:\n",
    "        return round(accuracy_score(y_test.get(), yhat.get()) * 100, 6)\n",
    "    else:\n",
    "        return round(accuracy_score(y_test, yhat) * 100 , 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegression:\n",
    "    # private:\n",
    "    def __init__(self, eta, solver='base', iterations=20, C1=0.0, C2=0.0, line_iters=0, batch_size=0):\n",
    "        self.eta = eta\n",
    "        self.solver = solver\n",
    "        self.iters = iterations\n",
    "        self.C1 = C1\n",
    "        self.C2 = C2\n",
    "        self.line_iters = line_iters\n",
    "        self.batch_size = batch_size\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "\n",
    "        solvers = ['base', 'line_search', 'stochastic', 'mini_batch', 'newton']\n",
    "        if solver not in solvers:\n",
    "            raise ValueError('solver %s is not one of %s' % (solver, solvers))\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "    \n",
    "    # convenience, private and static:\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return xp.hstack((xp.ones((X.shape[0], 1)), X)) # add bias term\n",
    "\n",
    "    # this defines the function with the first input to be optimized\n",
    "    # therefore eta will be optimized, with all inputs constant\n",
    "    @staticmethod\n",
    "    def _line_search_objective_function(eta, X, y, w, grad):\n",
    "        wnew = w - grad * eta\n",
    "        g = expit(X @ wnew)\n",
    "\n",
    "        if USE_GPU:\n",
    "            g = g.get()\n",
    "            y = y.get()\n",
    "\n",
    "        # has to be run on the CPU because of the use of the ma module\n",
    "        return -np.sum(ma.log(g[y == 1])) - ma.sum(np.log(1 - g[y == 0]))\n",
    "\n",
    "    def _add_regularization(self, grad):\n",
    "        L1 = self.C1 * xp.sign(self.w_[1:])\n",
    "        L2 = self.C2 -2 * self.w_[1:]\n",
    "        grad[1:] += L1 + L2\n",
    "\n",
    "        return grad\n",
    "\n",
    "    def _get_gradient(self, X, y):\n",
    "        match self.solver:\n",
    "            case 'base' | 'line_search':\n",
    "                ydiff = y - self.predict_proba(X, add_bias=False).ravel() # get y difference\n",
    "                # scale ydiff by ratio of positive to negative samples\n",
    "                # ydiff *= (y == 1).sum() / (y == 0).sum() * 0.5\n",
    "                gradient = xp.mean(X * ydiff[:, xp.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "            case 'stochastic':\n",
    "                idx = int(np.random.rand() * len(y)) # grab random instance\n",
    "                ydiff = y[idx] - self.predict_proba(X[idx], add_bias=False) # get y difference\n",
    "                gradient = X[idx] * ydiff[:, xp.newaxis] # make ydiff a column vector and multiply through\n",
    "            case 'mini_batch':\n",
    "                idx = np.random.choice(len(y), size=self.batch_size, replace=False) # grab random instance\n",
    "                ydiff = y[idx] - self.predict_proba(X[idx], add_bias=False).ravel() # get y difference\n",
    "                gradient = xp.mean(X[idx] * ydiff[:, xp.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "            case 'newton':\n",
    "                g = self.predict_proba(X, add_bias=False).ravel() # get sigmoid value for all classes\n",
    "                \n",
    "                # the hessian has no L1 regularization and L2 will only be included if C2 is not 0\n",
    "                hessian = (X * (g * (1 - g))[:, xp.newaxis]).T @ X - (2 * self.C2) # calculate the hessian\n",
    "\n",
    "                # I swapped X.T @ np.diag(g*(1-g)) with (X * (g*(1-g))[:, xp.newaxis]).T\n",
    "                # They work the same but the latter does the diagonal multiplication using way less memory\n",
    "                # The reduction in memory allows the use of the GPU for the hessian calculation\n",
    "                # otherwise my GPU runs out of memory\n",
    "\n",
    "                ydiff = y - g # get y difference\n",
    "                gradient = xp.sum(X * ydiff[:, np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape) # make gradient a column vector\n",
    "        gradient = self._add_regularization(gradient)\n",
    "\n",
    "        # the hessian is special\n",
    "        if self.solver == 'newton':\n",
    "            return xp.linalg.pinv(hessian) @ gradient\n",
    "        else:\n",
    "            return gradient\n",
    "    \n",
    "    # public:\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = xp.zeros((num_features, 1)) # init weight vector to zeros\n",
    "        \n",
    "        match self.solver:\n",
    "            case 'base' | 'stochastic' | 'mini_batch' | 'newton':\n",
    "                # for as many as the max iterations\n",
    "                for _ in range(self.iters):\n",
    "                    gradient = self._get_gradient(Xb, y)\n",
    "                    self.w_ += gradient * self.eta # multiply by learning rate\n",
    "\n",
    "            case 'line_search':\n",
    "                for _ in range(self.iters):\n",
    "                    gradient = -self._get_gradient(Xb, y)\n",
    "                    # minimization inopposite direction\n",
    "                    \n",
    "                    # do line search in gradient direction, using scipy function\n",
    "                    opts = {'maxiter':self.line_iters} # unclear exactly what this should be\n",
    "                    res = minimize_scalar(self._line_search_objective_function, # objective function to optimize\n",
    "                                        bounds=(0,self.eta * 10), #bounds to optimize\n",
    "                                        args=(Xb, y, self.w_, gradient), # additional argument for objective function\n",
    "                                        method='bounded', # bounded optimization for speed\n",
    "                                        options=opts) # set max iterations\n",
    "                    \n",
    "                    eta = res.x # get optimal learning rate\n",
    "                    self.w_ -= gradient * eta # set new function values\n",
    "                    # subtract to minimize\n",
    "\n",
    "        \n",
    "\n",
    "    def predict_proba(self, X, add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X) > 0.5) #return the actual prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, eta, solver='base', iterations=20, C1=0.0, C2=0.0, line_iters=5, batch_size=5):\n",
    "        self.eta = eta\n",
    "        self.solver = solver\n",
    "        self.iters = iterations\n",
    "        self.C1 = C1\n",
    "        self.C2 = C2\n",
    "        self.line_iters = line_iters\n",
    "        self.batch_size = batch_size\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "\n",
    "        solvers = ['base', 'line_search', 'stochastic', 'mini_batch', 'newton']\n",
    "        if solver not in solvers:\n",
    "            raise ValueError('solver %s is not one of %s' % (solver, solvers))\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = xp.unique(y) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        # create a classifier for each class\n",
    "        for _ in range(num_unique_classes):\n",
    "            blr = BinaryLogisticRegression(self.eta, self.solver, self.iters, self.C1, self.C2, self.line_iters, self.batch_size)\n",
    "            self.classifiers_.append(blr)\n",
    "\n",
    "        # print('Classifiers created')\n",
    "\n",
    "        # train each classifier\n",
    "        for blr, yval, i in zip(self.classifiers_, self.unique_, range(num_unique_classes)):\n",
    "            \n",
    "            y_binary = (y == yval)\n",
    "            blr.fit(X, y_binary)\n",
    "            # print('Classifier Fitted %d of %d' % (i + 1, num_unique_classes), end='\\r')\n",
    "        # print('All Classifiers Fitted        ')\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = xp.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        probs = []\n",
    "        for blr in self.classifiers_:\n",
    "            probs.append(blr.predict_proba(X)) # get probability for each classifier\n",
    "        \n",
    "        return xp.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return self.unique_[xp.argmax(self.predict_proba(X), axis=1)] # take argmax along row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of:  52.106 %\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(.1, solver='base', iterations=500, C1=0.01, C2=0.01)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "yhat = lr.predict(X_test)\n",
    "\n",
    "print_accuracy(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of:  53.432 %\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(.1, solver='line_search', iterations=100, C1=0.01, C2=0.01, line_iters=2)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "yhat = lr.predict(X_test)\n",
    "\n",
    "print_accuracy(y_test, yhat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of:  53.224 %\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(.1, solver='line_search', iterations=100, C1=0.01, C2=0.01, line_iters=5)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "yhat = lr.predict(X_test)\n",
    "\n",
    "print_accuracy(y_test, yhat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of:  41.134 %\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(.001, solver='stochastic', iterations=1000, C1=0.01, C2=0.01)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "yhat = lr.predict(X_test)\n",
    "\n",
    "print_accuracy(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of:  46.75 %\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(.1, solver='mini_batch', iterations=100, C1=0.01, C2=0.01, batch_size=2000)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "yhat = lr.predict(X_test)\n",
    "\n",
    "print_accuracy(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of:  63.807 %\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(.1, solver='newton', iterations=5, C1=0.01, C2=0.01)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "yhat = lr.predict(X_test)\n",
    "\n",
    "print_accuracy(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base eta: 0.001 C1: 0.0 C2: 0.0 iters: 100 accuracy: 40.198 time: 583ms\n",
      "base eta: 0.001 C1: 0.01 C2: 0.0 iters: 100 accuracy: 47.27 time: 298ms\n",
      "base eta: 0.001 C1: 0.0 C2: 0.01 iters: 100 accuracy: 40.198 time: 299ms\n",
      "base eta: 0.001 C1: 0.01 C2: 0.01 iters: 100 accuracy: 43.63 time: 298ms\n",
      "base eta: 0.01 C1: 0.0 C2: 0.0 iters: 100 accuracy: 41.68 time: 298ms\n",
      "base eta: 0.01 C1: 0.01 C2: 0.0 iters: 100 accuracy: 46.23 time: 298ms\n",
      "base eta: 0.01 C1: 0.0 C2: 0.01 iters: 100 accuracy: 41.68 time: 298ms\n",
      "base eta: 0.01 C1: 0.01 C2: 0.01 iters: 100 accuracy: 46.464 time: 298ms\n",
      "base eta: 0.1 C1: 0.0 C2: 0.0 iters: 100 accuracy: 46.958 time: 298ms\n",
      "base eta: 0.1 C1: 0.01 C2: 0.0 iters: 100 accuracy: 51.664 time: 298ms\n",
      "base eta: 0.1 C1: 0.0 C2: 0.01 iters: 100 accuracy: 46.932 time: 298ms\n",
      "base eta: 0.1 C1: 0.01 C2: 0.01 iters: 100 accuracy: 48.856 time: 299ms\n",
      "base eta: 0.001 C1: 0.0 C2: 0.0 iters: 500 accuracy: 40.952 time: 1632ms\n",
      "base eta: 0.001 C1: 0.01 C2: 0.0 iters: 500 accuracy: 48.44 time: 1628ms\n",
      "base eta: 0.001 C1: 0.0 C2: 0.01 iters: 500 accuracy: 40.952 time: 1674ms\n",
      "base eta: 0.001 C1: 0.01 C2: 0.01 iters: 500 accuracy: 44.93 time: 1630ms\n",
      "base eta: 0.01 C1: 0.0 C2: 0.0 iters: 500 accuracy: 45.476 time: 1640ms\n",
      "base eta: 0.01 C1: 0.01 C2: 0.0 iters: 500 accuracy: 49.22 time: 1636ms\n",
      "base eta: 0.01 C1: 0.0 C2: 0.01 iters: 500 accuracy: 45.476 time: 1630ms\n",
      "base eta: 0.01 C1: 0.01 C2: 0.01 iters: 500 accuracy: 49.818 time: 1630ms\n",
      "base eta: 0.1 C1: 0.0 C2: 0.0 iters: 500 accuracy: 47.738 time: 1644ms\n",
      "base eta: 0.1 C1: 0.01 C2: 0.0 iters: 500 accuracy: 51.56 time: 1664ms\n",
      "base eta: 0.1 C1: 0.0 C2: 0.01 iters: 500 accuracy: 47.79 time: 1632ms\n",
      "base eta: 0.1 C1: 0.01 C2: 0.01 iters: 500 accuracy: 52.106 time: 1634ms\n",
      "base eta: 0.001 C1: 0.0 C2: 0.0 iters: 1000 accuracy: 41.68 time: 3293ms\n",
      "base eta: 0.001 C1: 0.01 C2: 0.0 iters: 1000 accuracy: 46.256 time: 3345ms\n",
      "base eta: 0.001 C1: 0.0 C2: 0.01 iters: 1000 accuracy: 41.68 time: 3299ms\n",
      "base eta: 0.001 C1: 0.01 C2: 0.01 iters: 1000 accuracy: 46.438 time: 3298ms\n",
      "base eta: 0.01 C1: 0.0 C2: 0.0 iters: 1000 accuracy: 46.958 time: 3294ms\n",
      "base eta: 0.01 C1: 0.01 C2: 0.0 iters: 1000 accuracy: 51.69 time: 3295ms\n",
      "base eta: 0.01 C1: 0.0 C2: 0.01 iters: 1000 accuracy: 46.932 time: 3340ms\n",
      "base eta: 0.01 C1: 0.01 C2: 0.01 iters: 1000 accuracy: 48.934 time: 3444ms\n",
      "base eta: 0.1 C1: 0.0 C2: 0.0 iters: 1000 accuracy: 47.79 time: 3396ms\n",
      "base eta: 0.1 C1: 0.01 C2: 0.0 iters: 1000 accuracy: 51.56 time: 3355ms\n",
      "base eta: 0.1 C1: 0.0 C2: 0.01 iters: 1000 accuracy: 47.816 time: 3376ms\n",
      "base eta: 0.1 C1: 0.01 C2: 0.01 iters: 1000 accuracy: 53.016 time: 3345ms\n",
      "line_search eta: 0.001 C1: 0.0 C2: 0.0 iters: 20 accuracy: 40.328 time: 491ms\n",
      "line_search eta: 0.001 C1: 0.01 C2: 0.0 iters: 20 accuracy: 47.426 time: 404ms\n",
      "line_search eta: 0.001 C1: 0.0 C2: 0.01 iters: 20 accuracy: 40.328 time: 459ms\n",
      "line_search eta: 0.001 C1: 0.01 C2: 0.01 iters: 20 accuracy: 43.812 time: 447ms\n",
      "line_search eta: 0.01 C1: 0.0 C2: 0.0 iters: 20 accuracy: 43.136 time: 501ms\n",
      "line_search eta: 0.01 C1: 0.01 C2: 0.0 iters: 20 accuracy: 46.958 time: 477ms\n",
      "line_search eta: 0.01 C1: 0.0 C2: 0.01 iters: 20 accuracy: 43.162 time: 496ms\n",
      "line_search eta: 0.01 C1: 0.01 C2: 0.01 iters: 20 accuracy: 47.478 time: 504ms\n",
      "line_search eta: 0.1 C1: 0.0 C2: 0.0 iters: 20 accuracy: 45.658 time: 523ms\n",
      "line_search eta: 0.1 C1: 0.01 C2: 0.0 iters: 20 accuracy: 49.506 time: 491ms\n",
      "line_search eta: 0.1 C1: 0.0 C2: 0.01 iters: 20 accuracy: 40.276 time: 495ms\n",
      "line_search eta: 0.1 C1: 0.01 C2: 0.01 iters: 20 accuracy: 51.43 time: 540ms\n",
      "line_search eta: 0.001 C1: 0.0 C2: 0.0 iters: 100 accuracy: 41.524 time: 2557ms\n",
      "line_search eta: 0.001 C1: 0.01 C2: 0.0 iters: 100 accuracy: 46.256 time: 2441ms\n",
      "line_search eta: 0.001 C1: 0.0 C2: 0.01 iters: 100 accuracy: 41.576 time: 2710ms\n",
      "line_search eta: 0.001 C1: 0.01 C2: 0.01 iters: 100 accuracy: 46.126 time: 2711ms\n",
      "line_search eta: 0.01 C1: 0.0 C2: 0.0 iters: 100 accuracy: 46.698 time: 2233ms\n",
      "line_search eta: 0.01 C1: 0.01 C2: 0.0 iters: 100 accuracy: 51.95 time: 2404ms\n",
      "line_search eta: 0.01 C1: 0.0 C2: 0.01 iters: 100 accuracy: 46.724 time: 2220ms\n",
      "line_search eta: 0.01 C1: 0.01 C2: 0.01 iters: 100 accuracy: 49.038 time: 2237ms\n",
      "line_search eta: 0.1 C1: 0.0 C2: 0.0 iters: 100 accuracy: 45.138 time: 2184ms\n",
      "line_search eta: 0.1 C1: 0.01 C2: 0.0 iters: 100 accuracy: 50.26 time: 2135ms\n",
      "line_search eta: 0.1 C1: 0.0 C2: 0.01 iters: 100 accuracy: 47.868 time: 2584ms\n",
      "line_search eta: 0.1 C1: 0.01 C2: 0.01 iters: 100 accuracy: 53.224 time: 2521ms\n",
      "line_search eta: 0.001 C1: 0.0 C2: 0.0 iters: 200 accuracy: 43.084 time: 4444ms\n",
      "line_search eta: 0.001 C1: 0.01 C2: 0.0 iters: 200 accuracy: 46.958 time: 4422ms\n",
      "line_search eta: 0.001 C1: 0.0 C2: 0.01 iters: 200 accuracy: 43.11 time: 4472ms\n",
      "line_search eta: 0.001 C1: 0.01 C2: 0.01 iters: 200 accuracy: 47.478 time: 4265ms\n",
      "line_search eta: 0.01 C1: 0.0 C2: 0.0 iters: 200 accuracy: 47.686 time: 4410ms\n",
      "line_search eta: 0.01 C1: 0.01 C2: 0.0 iters: 200 accuracy: 51.352 time: 4307ms\n",
      "line_search eta: 0.01 C1: 0.0 C2: 0.01 iters: 200 accuracy: 47.66 time: 4323ms\n",
      "line_search eta: 0.01 C1: 0.01 C2: 0.01 iters: 200 accuracy: 51.898 time: 4315ms\n",
      "line_search eta: 0.1 C1: 0.0 C2: 0.0 iters: 200 accuracy: 47.088 time: 4247ms\n",
      "line_search eta: 0.1 C1: 0.01 C2: 0.0 iters: 200 accuracy: 51.014 time: 4296ms\n",
      "line_search eta: 0.1 C1: 0.0 C2: 0.01 iters: 200 accuracy: 47.192 time: 4440ms\n",
      "line_search eta: 0.1 C1: 0.01 C2: 0.01 iters: 200 accuracy: 52.886 time: 4454ms\n",
      "stochastic eta: 0.001 C1: 0.0 C2: 0.0 iters: 200 accuracy: 28.575 time: 626ms\n",
      "stochastic eta: 0.001 C1: 0.01 C2: 0.0 iters: 200 accuracy: 19.241 time: 351ms\n",
      "stochastic eta: 0.001 C1: 0.0 C2: 0.01 iters: 200 accuracy: 23.193 time: 353ms\n",
      "stochastic eta: 0.001 C1: 0.01 C2: 0.01 iters: 200 accuracy: 33.021 time: 353ms\n",
      "stochastic eta: 0.01 C1: 0.0 C2: 0.0 iters: 200 accuracy: 20.099 time: 355ms\n",
      "stochastic eta: 0.01 C1: 0.01 C2: 0.0 iters: 200 accuracy: 39.158 time: 355ms\n",
      "stochastic eta: 0.01 C1: 0.0 C2: 0.01 iters: 200 accuracy: 43.032 time: 598ms\n",
      "stochastic eta: 0.01 C1: 0.01 C2: 0.01 iters: 200 accuracy: 27.847 time: 355ms\n",
      "stochastic eta: 0.1 C1: 0.0 C2: 0.0 iters: 200 accuracy: 19.449 time: 352ms\n",
      "stochastic eta: 0.1 C1: 0.01 C2: 0.0 iters: 200 accuracy: 26.001 time: 351ms\n",
      "stochastic eta: 0.1 C1: 0.0 C2: 0.01 iters: 200 accuracy: 26.521 time: 481ms\n",
      "stochastic eta: 0.1 C1: 0.01 C2: 0.01 iters: 200 accuracy: 14.275 time: 352ms\n",
      "stochastic eta: 0.001 C1: 0.0 C2: 0.0 iters: 1000 accuracy: 35.283 time: 1736ms\n",
      "stochastic eta: 0.001 C1: 0.01 C2: 0.0 iters: 1000 accuracy: 41.238 time: 3522ms\n",
      "stochastic eta: 0.001 C1: 0.0 C2: 0.01 iters: 1000 accuracy: 40.458 time: 2695ms\n",
      "stochastic eta: 0.001 C1: 0.01 C2: 0.01 iters: 1000 accuracy: 32.527 time: 1747ms\n",
      "stochastic eta: 0.01 C1: 0.0 C2: 0.0 iters: 1000 accuracy: 16.017 time: 1745ms\n",
      "stochastic eta: 0.01 C1: 0.01 C2: 0.0 iters: 1000 accuracy: 27.925 time: 2266ms\n",
      "stochastic eta: 0.01 C1: 0.0 C2: 0.01 iters: 1000 accuracy: 22.231 time: 2258ms\n",
      "stochastic eta: 0.01 C1: 0.01 C2: 0.01 iters: 1000 accuracy: 23.947 time: 1714ms\n",
      "stochastic eta: 0.1 C1: 0.0 C2: 0.0 iters: 1000 accuracy: 14.275 time: 1748ms\n",
      "stochastic eta: 0.1 C1: 0.01 C2: 0.0 iters: 1000 accuracy: 36.349 time: 1857ms\n",
      "stochastic eta: 0.1 C1: 0.0 C2: 0.01 iters: 1000 accuracy: 14.275 time: 2103ms\n",
      "stochastic eta: 0.1 C1: 0.01 C2: 0.01 iters: 1000 accuracy: 17.031 time: 1855ms\n",
      "stochastic eta: 0.001 C1: 0.0 C2: 0.0 iters: 2000 accuracy: 43.968 time: 6269ms\n",
      "stochastic eta: 0.001 C1: 0.01 C2: 0.0 iters: 2000 accuracy: 49.09 time: 4104ms\n",
      "stochastic eta: 0.001 C1: 0.0 C2: 0.01 iters: 2000 accuracy: 28.393 time: 4457ms\n",
      "stochastic eta: 0.001 C1: 0.01 C2: 0.01 iters: 2000 accuracy: 36.661 time: 4054ms\n",
      "stochastic eta: 0.01 C1: 0.0 C2: 0.0 iters: 2000 accuracy: 33.515 time: 3853ms\n",
      "stochastic eta: 0.01 C1: 0.01 C2: 0.0 iters: 2000 accuracy: 40.874 time: 3870ms\n",
      "stochastic eta: 0.01 C1: 0.0 C2: 0.01 iters: 2000 accuracy: 24.285 time: 3445ms\n",
      "stochastic eta: 0.01 C1: 0.01 C2: 0.01 iters: 2000 accuracy: 28.133 time: 3476ms\n",
      "stochastic eta: 0.1 C1: 0.0 C2: 0.0 iters: 2000 accuracy: 14.275 time: 3752ms\n",
      "stochastic eta: 0.1 C1: 0.01 C2: 0.0 iters: 2000 accuracy: 19.137 time: 3561ms\n",
      "stochastic eta: 0.1 C1: 0.0 C2: 0.01 iters: 2000 accuracy: 24.389 time: 4914ms\n",
      "stochastic eta: 0.1 C1: 0.01 C2: 0.01 iters: 2000 accuracy: 28.055 time: 3475ms\n",
      "mini_batch eta: 0.001 C1: 0.0 C2: 0.0 iters: 20 accuracy: 14.275 time: 93ms\n",
      "mini_batch eta: 0.001 C1: 0.01 C2: 0.0 iters: 20 accuracy: 34.867 time: 90ms\n",
      "mini_batch eta: 0.001 C1: 0.0 C2: 0.01 iters: 20 accuracy: 28.289 time: 94ms\n",
      "mini_batch eta: 0.001 C1: 0.01 C2: 0.01 iters: 20 accuracy: 16.979 time: 92ms\n",
      "mini_batch eta: 0.01 C1: 0.0 C2: 0.0 iters: 20 accuracy: 26.833 time: 91ms\n",
      "mini_batch eta: 0.01 C1: 0.01 C2: 0.0 iters: 20 accuracy: 28.575 time: 93ms\n",
      "mini_batch eta: 0.01 C1: 0.0 C2: 0.01 iters: 20 accuracy: 31.929 time: 93ms\n",
      "mini_batch eta: 0.01 C1: 0.01 C2: 0.01 iters: 20 accuracy: 24.753 time: 91ms\n",
      "mini_batch eta: 0.1 C1: 0.0 C2: 0.0 iters: 20 accuracy: 25.429 time: 95ms\n",
      "mini_batch eta: 0.1 C1: 0.01 C2: 0.0 iters: 20 accuracy: 24.779 time: 92ms\n",
      "mini_batch eta: 0.1 C1: 0.0 C2: 0.01 iters: 20 accuracy: 14.535 time: 91ms\n",
      "mini_batch eta: 0.1 C1: 0.01 C2: 0.01 iters: 20 accuracy: 14.483 time: 170ms\n",
      "mini_batch eta: 0.001 C1: 0.0 C2: 0.0 iters: 100 accuracy: 14.873 time: 444ms\n",
      "mini_batch eta: 0.001 C1: 0.01 C2: 0.0 iters: 100 accuracy: 30.109 time: 458ms\n",
      "mini_batch eta: 0.001 C1: 0.0 C2: 0.01 iters: 100 accuracy: 28.575 time: 706ms\n",
      "mini_batch eta: 0.001 C1: 0.01 C2: 0.01 iters: 100 accuracy: 29.095 time: 547ms\n",
      "mini_batch eta: 0.01 C1: 0.0 C2: 0.0 iters: 100 accuracy: 33.983 time: 446ms\n",
      "mini_batch eta: 0.01 C1: 0.01 C2: 0.0 iters: 100 accuracy: 26.001 time: 442ms\n",
      "mini_batch eta: 0.01 C1: 0.0 C2: 0.01 iters: 100 accuracy: 34.087 time: 503ms\n",
      "mini_batch eta: 0.01 C1: 0.01 C2: 0.01 iters: 100 accuracy: 44.618 time: 756ms\n",
      "mini_batch eta: 0.1 C1: 0.0 C2: 0.0 iters: 100 accuracy: 26.105 time: 447ms\n",
      "mini_batch eta: 0.1 C1: 0.01 C2: 0.0 iters: 100 accuracy: 14.275 time: 443ms\n",
      "mini_batch eta: 0.1 C1: 0.0 C2: 0.01 iters: 100 accuracy: 14.275 time: 442ms\n",
      "mini_batch eta: 0.1 C1: 0.01 C2: 0.01 iters: 100 accuracy: 29.251 time: 441ms\n",
      "mini_batch eta: 0.001 C1: 0.0 C2: 0.0 iters: 200 accuracy: 45.944 time: 890ms\n",
      "mini_batch eta: 0.001 C1: 0.01 C2: 0.0 iters: 200 accuracy: 36.843 time: 893ms\n",
      "mini_batch eta: 0.001 C1: 0.0 C2: 0.01 iters: 200 accuracy: 41.186 time: 1086ms\n",
      "mini_batch eta: 0.001 C1: 0.01 C2: 0.01 iters: 200 accuracy: 35.725 time: 913ms\n",
      "mini_batch eta: 0.01 C1: 0.0 C2: 0.0 iters: 200 accuracy: 34.737 time: 1342ms\n",
      "mini_batch eta: 0.01 C1: 0.01 C2: 0.0 iters: 200 accuracy: 40.64 time: 1420ms\n",
      "mini_batch eta: 0.01 C1: 0.0 C2: 0.01 iters: 200 accuracy: 40.978 time: 960ms\n",
      "mini_batch eta: 0.01 C1: 0.01 C2: 0.01 iters: 200 accuracy: 34.711 time: 917ms\n",
      "mini_batch eta: 0.1 C1: 0.0 C2: 0.0 iters: 200 accuracy: 27.847 time: 923ms\n",
      "mini_batch eta: 0.1 C1: 0.01 C2: 0.0 iters: 200 accuracy: 18.669 time: 1057ms\n",
      "mini_batch eta: 0.1 C1: 0.0 C2: 0.01 iters: 200 accuracy: 29.589 time: 951ms\n",
      "mini_batch eta: 0.1 C1: 0.01 C2: 0.01 iters: 200 accuracy: 14.483 time: 910ms\n",
      "newton eta: 0.001 C1: 0.0 C2: 0.0 iters: 1 accuracy: 63.755 time: 168ms\n",
      "newton eta: 0.001 C1: 0.01 C2: 0.0 iters: 1 accuracy: 63.755 time: 106ms\n",
      "newton eta: 0.001 C1: 0.0 C2: 0.01 iters: 1 accuracy: 63.755 time: 73ms\n",
      "newton eta: 0.001 C1: 0.01 C2: 0.01 iters: 1 accuracy: 63.755 time: 73ms\n",
      "newton eta: 0.01 C1: 0.0 C2: 0.0 iters: 1 accuracy: 63.755 time: 126ms\n",
      "newton eta: 0.01 C1: 0.01 C2: 0.0 iters: 1 accuracy: 63.755 time: 130ms\n",
      "newton eta: 0.01 C1: 0.0 C2: 0.01 iters: 1 accuracy: 63.755 time: 103ms\n",
      "newton eta: 0.01 C1: 0.01 C2: 0.01 iters: 1 accuracy: 63.755 time: 132ms\n",
      "newton eta: 0.1 C1: 0.0 C2: 0.0 iters: 1 accuracy: 63.755 time: 125ms\n",
      "newton eta: 0.1 C1: 0.01 C2: 0.0 iters: 1 accuracy: 63.755 time: 66ms\n",
      "newton eta: 0.1 C1: 0.0 C2: 0.01 iters: 1 accuracy: 63.755 time: 64ms\n",
      "newton eta: 0.1 C1: 0.01 C2: 0.01 iters: 1 accuracy: 63.755 time: 65ms\n",
      "newton eta: 0.001 C1: 0.0 C2: 0.0 iters: 5 accuracy: 63.755 time: 320ms\n",
      "newton eta: 0.001 C1: 0.01 C2: 0.0 iters: 5 accuracy: 63.755 time: 324ms\n",
      "newton eta: 0.001 C1: 0.0 C2: 0.01 iters: 5 accuracy: 63.755 time: 320ms\n",
      "newton eta: 0.001 C1: 0.01 C2: 0.01 iters: 5 accuracy: 63.755 time: 598ms\n",
      "newton eta: 0.01 C1: 0.0 C2: 0.0 iters: 5 accuracy: 63.599 time: 380ms\n",
      "newton eta: 0.01 C1: 0.01 C2: 0.0 iters: 5 accuracy: 63.651 time: 351ms\n",
      "newton eta: 0.01 C1: 0.0 C2: 0.01 iters: 5 accuracy: 63.625 time: 421ms\n",
      "newton eta: 0.01 C1: 0.01 C2: 0.01 iters: 5 accuracy: 63.651 time: 322ms\n",
      "newton eta: 0.1 C1: 0.0 C2: 0.0 iters: 5 accuracy: 63.807 time: 315ms\n",
      "newton eta: 0.1 C1: 0.01 C2: 0.0 iters: 5 accuracy: 63.807 time: 365ms\n",
      "newton eta: 0.1 C1: 0.0 C2: 0.01 iters: 5 accuracy: 63.807 time: 448ms\n",
      "newton eta: 0.1 C1: 0.01 C2: 0.01 iters: 5 accuracy: 63.807 time: 337ms\n",
      "newton eta: 0.001 C1: 0.0 C2: 0.0 iters: 10 accuracy: 63.729 time: 897ms\n",
      "newton eta: 0.001 C1: 0.01 C2: 0.0 iters: 10 accuracy: 63.755 time: 656ms\n",
      "newton eta: 0.001 C1: 0.0 C2: 0.01 iters: 10 accuracy: 63.729 time: 774ms\n",
      "newton eta: 0.001 C1: 0.01 C2: 0.01 iters: 10 accuracy: 63.755 time: 1210ms\n",
      "newton eta: 0.01 C1: 0.0 C2: 0.0 iters: 10 accuracy: 63.547 time: 668ms\n",
      "newton eta: 0.01 C1: 0.01 C2: 0.0 iters: 10 accuracy: 63.547 time: 884ms\n",
      "newton eta: 0.01 C1: 0.0 C2: 0.01 iters: 10 accuracy: 63.573 time: 772ms\n",
      "newton eta: 0.01 C1: 0.01 C2: 0.01 iters: 10 accuracy: 63.547 time: 723ms\n",
      "newton eta: 0.1 C1: 0.0 C2: 0.0 iters: 10 accuracy: 64.457 time: 665ms\n",
      "newton eta: 0.1 C1: 0.01 C2: 0.0 iters: 10 accuracy: 64.483 time: 730ms\n",
      "newton eta: 0.1 C1: 0.0 C2: 0.01 iters: 10 accuracy: 64.483 time: 820ms\n",
      "newton eta: 0.1 C1: 0.01 C2: 0.01 iters: 10 accuracy: 64.483 time: 890ms\n"
     ]
    }
   ],
   "source": [
    "solvers = ['base', 'line_search', 'stochastic', 'mini_batch', 'newton']\n",
    "base_num_iters = [100, 20, 200, 20, 1] # each solver has a different number of base iterations\n",
    "iter_multipliers = [1, 5, 10]\n",
    "etas = [.001, .01, .1]\n",
    "Cs = [(0.0, 0.0), (0.01, 0.0), (0.0, 0.01), (0.01, 0.01)]\n",
    "\n",
    "results = []\n",
    "\n",
    "for solver in solvers:\n",
    "    for iter_multiplier in iter_multipliers:\n",
    "        for eta in etas:\n",
    "            for C1, C2 in Cs:\n",
    "                # time the training in milliseconds\n",
    "                start = round(time.time() * 1000)\n",
    "\n",
    "                lr = LogisticRegression(eta, solver=solver, iterations=(base_num_iters[solvers.index(solver)] * iter_multiplier), C1=C1, C2=C2)\n",
    "                lr.fit(X_train, y_train)\n",
    "                yhat = lr.predict(X_test)\n",
    "\n",
    "                end = round(time.time() * 1000)\n",
    "\n",
    "                settings = solver + ' eta: ' + str(eta) + ' C1: ' + str(C1) + ' C2: ' + str(C2) + ' iters: ' + str(base_num_iters[solvers.index(solver)] * iter_multiplier)\n",
    "                accuracy = get_accuracy(y_test, yhat)\n",
    "                time_taken = end - start\n",
    "\n",
    "                # store the settings, accuracy, and time\n",
    "                print(settings + ' accuracy: ' + str(accuracy) + ' time: ' + str(time_taken) + 'ms')\n",
    "                results.append((settings, accuracy, time_taken))\n",
    "\n",
    "\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
